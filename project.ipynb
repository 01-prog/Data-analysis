{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530e011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode\n",
    "from typing import List, Dict, Optional, Set\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# vinted_scraper.py\n",
    "# Requires: requests, bs4\n",
    "# Optional: selenium (if dynamic rendering required)\n",
    "\n",
    "\n",
    "# Reuse notebook variables if present\n",
    "BASE_URL = 'https://www.vinted.fr'\n",
    "START_URL = f'{BASE_URL}/vetements?search_text=jean'\n",
    "USER_AGENT = 'Mozilla/5.0 (compatible; VintedScraper/1.0; +https://example.com/bot)'\n",
    "session = requests.Session()\n",
    "session.headers.update({'User-Agent': USER_AGENT})\n",
    "rp = None\n",
    "REQUEST_DELAY = (1.0, 3.0)\n",
    "MAX_PAGES = 50\n",
    "LOG_LEVEL = 20\n",
    "OUTPUT_CSV = 'vinted_products.csv'\n",
    "OUTPUT_JSONL = 'vinted_products.jsonl'\n",
    "\n",
    "logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "session.headers.update({'User-Agent': USER_AGENT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ce3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polite_get(url: str, timeout: int = 15) -> Optional[requests.Response]:\n",
    "    \"\"\"Fetch URL while respecting robots.txt (if rp provided) and random delay.\"\"\"\n",
    "    if rp is not None:\n",
    "        try:\n",
    "            if not rp.can_fetch(USER_AGENT, url):\n",
    "                logger.warning(\"Blocked by robots.txt: %s\", url)\n",
    "                return None\n",
    "        except Exception:\n",
    "            # If robotparser misconfigured, continue but log\n",
    "            logger.debug(\"robots parser error for %s\", url)\n",
    "    try:\n",
    "        resp = session.get(url, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            # polite delay\n",
    "            time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "            return resp\n",
    "        logger.warning(\"GET %s -> status %s\", url, resp.status_code)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Request failed: %s\", e)\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_page_url(base: str, page: int) -> str:\n",
    "    \"\"\"Add or replace page param in a URL's query string.\"\"\"\n",
    "    parsed = urlparse(base)\n",
    "    qs = parse_qs(parsed.query)\n",
    "    qs['page'] = [str(page)]\n",
    "    new_query = urlencode({k: v[0] for k, v in qs.items()})\n",
    "    return parsed._replace(query=new_query).geturl()\n",
    "\n",
    "\n",
    "PRODUCT_URL_PATTERNS = [r'/items/', r'/item/', r'/v/', r'/listing/']\n",
    "\n",
    "\n",
    "def looks_like_product(href: str) -> bool:\n",
    "    if not href:\n",
    "        return False\n",
    "    parsed = urlparse(href)\n",
    "    path = parsed.path\n",
    "    for p in PRODUCT_URL_PATTERNS:\n",
    "        if re.search(p, path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_links_from_listing(html: str) -> Set[str]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # normalize relative URLs\n",
    "        full = urljoin(BASE_URL, href)\n",
    "        if looks_like_product(full):\n",
    "            # strip fragment and query params that aren't needed\n",
    "            u = full.split('#')[0]\n",
    "            links.add(u)\n",
    "    return links\n",
    "\n",
    "\n",
    "def parse_product_page(html: str, url: str) -> Dict:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    data = {'url': url}\n",
    "\n",
    "    # Try JSON-LD first\n",
    "    try:\n",
    "        for script in soup.find_all('script', type='application/ld+json'):\n",
    "            try:\n",
    "                j = json.loads(script.string or '')\n",
    "            except Exception:\n",
    "                continue\n",
    "            # j might be a list\n",
    "            items = j if isinstance(j, list) else [j]\n",
    "            for item in items:\n",
    "                if isinstance(item, dict) and item.get('@type', '').lower() == 'product':\n",
    "                    data['title'] = item.get('name') or data.get('title')\n",
    "                    data['description'] = item.get('description') or data.get('description')\n",
    "                    images = item.get('image')\n",
    "                    if isinstance(images, list):\n",
    "                        data['images'] = images\n",
    "                    elif isinstance(images, str):\n",
    "                        data['images'] = [images]\n",
    "                    offers = item.get('offers') or {}\n",
    "                    if isinstance(offers, dict):\n",
    "                        data['price'] = offers.get('price')\n",
    "                        data['currency'] = offers.get('priceCurrency')\n",
    "                    # found JSON-LD product; good enough\n",
    "                    break\n",
    "            if 'title' in data:\n",
    "                break\n",
    "    except Exception:\n",
    "        logger.debug(\"JSON-LD parse error\", exc_info=True)\n",
    "\n",
    "    # Fallback to meta tags\n",
    "    if 'title' not in data:\n",
    "        og_title = soup.find('meta', property='og:title')\n",
    "        if og_title and og_title.get('content'):\n",
    "            data['title'] = og_title['content']\n",
    "    if 'description' not in data:\n",
    "        og_desc = soup.find('meta', property='og:description')\n",
    "        if og_desc and og_desc.get('content'):\n",
    "            data['description'] = og_desc['content']\n",
    "    if 'images' not in data:\n",
    "        og_image = soup.find('meta', property='og:image')\n",
    "        if og_image and og_image.get('content'):\n",
    "            data['images'] = [og_image['content']]\n",
    "\n",
    "    # price meta\n",
    "    if 'price' not in data:\n",
    "        price_meta = soup.find('meta', attrs={'property': 'product:price:amount'})\n",
    "        if price_meta and price_meta.get('content'):\n",
    "            data['price'] = price_meta['content']\n",
    "    if 'currency' not in data:\n",
    "        cur_meta = soup.find('meta', attrs={'property': 'product:price:currency'})\n",
    "        if cur_meta and cur_meta.get('content'):\n",
    "            data['currency'] = cur_meta['content']\n",
    "\n",
    "    # Extract textual details heuristically\n",
    "    try:\n",
    "        # item id from url digits\n",
    "        m = re.search(r'(\\d{4,})', url)\n",
    "        if m:\n",
    "            data['item_id'] = m.group(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # images from gallery\n",
    "    imgs = []\n",
    "    for img in soup.select('img'):\n",
    "        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')\n",
    "        if src and src.startswith('http'):\n",
    "            imgs.append(src)\n",
    "    if imgs:\n",
    "        data.setdefault('images', imgs)\n",
    "\n",
    "    # Seller info heuristics\n",
    "    seller = {}\n",
    "    seller_a = soup.find('a', href=re.compile(r'/membre/|/member/'))\n",
    "    if seller_a:\n",
    "        seller['name'] = seller_a.get_text(strip=True)\n",
    "        seller['url'] = urljoin(BASE_URL, seller_a['href'])\n",
    "    if seller:\n",
    "        data['seller'] = seller\n",
    "\n",
    "    # Additional fields by selectors (best-effort)\n",
    "    # price text\n",
    "    if 'price' not in data:\n",
    "        price_el = soup.select_one('[data-testid=\"price\"], .price, .ProductPrice, .item-price')\n",
    "        if price_el:\n",
    "            data['price'] = price_el.get_text(strip=True)\n",
    "    # size, brand, condition - try to find label/value pairs\n",
    "    attrs = {}\n",
    "    for li in soup.select('.item-attributes li, .details-list li, .CharacteristicList li'):\n",
    "        txt = li.get_text(separator=' ', strip=True)\n",
    "        if ':' in txt:\n",
    "            k, v = txt.split(':', 1)\n",
    "            attrs[k.strip().lower()] = v.strip()\n",
    "    if attrs:\n",
    "        data.setdefault('attributes', attrs)\n",
    "\n",
    "    data['scraped_at'] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_results(items: List[Dict], csv_path: str = OUTPUT_CSV, jsonl_path: str = OUTPUT_JSONL):\n",
    "    # JSONL\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for it in items:\n",
    "            f.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "    # CSV (flatten some fields)\n",
    "    fieldnames = [\n",
    "        'item_id', 'title', 'price', 'currency', 'url', 'seller', 'images', 'description', 'scraped_at'\n",
    "    ]\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for it in items:\n",
    "            row = {\n",
    "                'item_id': it.get('item_id'),\n",
    "                'title': it.get('title'),\n",
    "                'price': it.get('price'),\n",
    "                'currency': it.get('currency'),\n",
    "                'url': it.get('url'),\n",
    "                'seller': it.get('seller', {}).get('name') if isinstance(it.get('seller'), dict) else '',\n",
    "                'images': ';'.join(it.get('images', [])) if it.get('images') else '',\n",
    "                'description': it.get('description', '')[:1000],\n",
    "                'scraped_at': it.get('scraped_at'),\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def scrape_vinted(start_url: str = START_URL, max_pages: int = MAX_PAGES) -> List[Dict]:\n",
    "    all_products = []\n",
    "    seen_urls = set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        page_url = make_page_url(start_url, page)\n",
    "        logger.info(\"Fetching listing page %d: %s\", page, page_url)\n",
    "        resp = polite_get(page_url)\n",
    "        if not resp:\n",
    "            logger.info(\"Stopping: failed to fetch page %d\", page)\n",
    "            break\n",
    "        links = extract_links_from_listing(resp.text)\n",
    "        links = [l for l in links if l not in seen_urls]\n",
    "        logger.info(\"Found %d new product links on page %d\", len(links), page)\n",
    "        if not links:\n",
    "            # nothing new, stop\n",
    "            break\n",
    "        for prod_url in links:\n",
    "            seen_urls.add(prod_url)\n",
    "            logger.info(\"Fetching product: %s\", prod_url)\n",
    "            presp = polite_get(prod_url)\n",
    "            if not presp:\n",
    "                logger.warning(\"Failed product fetch: %s\", prod_url)\n",
    "                continue\n",
    "            pdata = parse_product_page(presp.text, prod_url)\n",
    "            all_products.append(pdata)\n",
    "        # quick stop if reached many products (optional)\n",
    "    logger.info(\"Scraped %d products\", len(all_products))\n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = scrape_vinted()\n",
    "save_results(items)\n",
    "logger.info(\"Saved %d items to %s and %s\", len(items), OUTPUT_CSV, OUTPUT_JSONL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
